# 语音识别

## 语音识别介绍

![1559271453111](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1559271453111.png)



Pre-processing: convert the speech signal from analog to digital, pre-emphasis, vad

The main goal of the preprocessing phase is to minimize the negative effects of the environment on the speech signal and to work on improving it to perform a better recognition for this signal in the upcoming stages. There are several techniques and operations could be performed during this stage[19]. **Converting the speech signal from analog to digital is considered one of the most important steps in this stage. This can be done by sampling the signal using a proper sampling frequency, such as 16kHz or 8 kHz, and applying the quantization process[19].** Also, one of the first improvements that could be applied to the signal at this phase is the **pre-emphasis processing. The goal of pre-emphasis filter is to increase the amplitude of the signals that have high frequencies and reduce the amplitude of the signals that have low frequencies (e.g. 100Hz)** [16,17].
In general, the normal ASR system works to assign each sound in the speech to some amount of probability[16]. So, any kind of noises that could happen during the recording operation could lead to inserting word/words to the output hypotheses. The preprocessing stage tries to avoid this by applying the speech/non-speech segmentation operation. **This operation works to delete some part of the recording such as removing the part that is located between the beginning of the recording and the time when the actual speech starts, and the end of the recording after the actual speech finishes[16]. This segmentation method is known by the end point detection**. 



**ASR　probability　theory：**

Speech recognition systems aim to　hypothesize the ideal discrete character among the whole given sequence for the acoustic　input, O, where O can be represented as [17] :

![1559272455683](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1559272455683.png)

Also, the selected symbol sequence can be represented as: 

![1559272421969](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1559272421969.png)

So the basic goal of the speech recognition systems can be represented as: 

![1559272496745](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1559272496745.png)



![1559272622911](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1559272622911.png)



**CMVN:**

![1559294334549](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1559294334549.png)

**时频幅三维语谱图**

![img](https://images2015.cnblogs.com/blog/1033445/201610/1033445-20161030153052593-277461579.png)

![img](https://images2015.cnblogs.com/blog/1033445/201610/1033445-20161030153137125-1957787712.png)

![img](https://images2015.cnblogs.com/blog/1033445/201610/1033445-20161030153245953-1222738146.png)



**Decoding:**

The decoding process works to finding the list of words that can be said. These words could be found in the dictionary part, which is a list of all the possible words plus their phonemes. Another important part which the acoustic model which has probability function, that is represented by the Gaussian mixture and the likelihood of each observed component P(O|W). However, the language modeling is not so important in comparison to The acoustic modeling, but it is used because it works to raise the words' accuracy and to
correct the words' grammar. 





[语音识别的研究和发展](<http://www.cnetnews.com.cn/2017/0821/3097159.shtml>)

[语音识别](<https://www.sohu.com/a/160550466_197042>)



利用机器进行语音识别的第一步就是将声波输入到机器中，即将声波转换成数字。声波是一维的，它在每一时刻都有一个基于其高度的值，为了将声波转换成数字，我们只记录声波在等距点的高度，即采样。由于采样定理（Nyquist theorem），我们可以利用采样完美重建原始声波。

![img](http://mmbiz.qpic.cn/mmbiz_png/M2r7SV1ewOutuOcUCyLnkp7v6IkPhV1nz1uW40hg9LItxpgB2zN6hzADWOllgOaB2hSF2ibJRStojHiabuVvrPVQ/0?wx_fmt=png)



![img](http://mmbiz.qpic.cn/mmbiz_png/M2r7SV1ewOutuOcUCyLnkp7v6IkPhV1nxGf7SpCEYTibqAKhU4Z0ftShyDKHxdnv5PA6DXN7YzibhT8cQADCWw5w/0?wx_fmt=png)



![img](http://mmbiz.qpic.cn/mmbiz_gif/M2r7SV1ewOutuOcUCyLnkp7v6IkPhV1nmYy5dGUxl4RsGLeBb2DZPvuvGk3E3F6ib7e7OSFIo1nYaibvib517nf9g/0?wx_fmt=gif)

我们可以将采样到的数字直接输入到神经网络，但这样进行语音识别很困难，因此，需要一些预处理。

将这些数字绘制成简单的折线图，形成如下图的原始声波的大致形状：

![1560072315154](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1560072315154.png)



在这一小段语音中，即使非常短暂，它也是由不同频率的声音复杂的组合在一起的，其中有一些低音，有一些中音，甚至有些高音。就是这些不同频率的声音混合在一起，才组成了人类的语音。

利用傅里叶变换将复杂的声波分解成简单的声波，将每个频段所包含的能量相加，最终得到，从低频到高频，每个频段的重要程度。如下面的频谱图。

![1560074185166](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1560074185166.png)



对于神经网络来说，相比于原始声波，频谱图更容易寻找规律。



声学模型：给定文本，它的发音会是什么样。

语言模型：句子的先验概率；一句话像不像话；在什么都不知道的情况下，你能听到这句话的概率有多大。



我们都知道MFCC很好的表达了语音的特征，但只是静态的特征。提取**动态特征**，一般都采用一阶二阶差分，但一阶二阶差分究竟表示什么，什么含义：

**一阶差分**就是离散函数中连续相邻两项之差；定义X(k),则Y(k)=X(k+1)-X(k)就是此函数的一阶差分，物理意义就是当前语音帧与前一帧之间的关系， 体现帧与帧（**相邻两帧**）之间的联系；

在一阶差分的基础上，Z(k)=Y(k+1)-Y(k)=X(k+2)-2*X(k+1)+X(k)为次函数的二阶差分。二阶差分表示的是一阶差分与一阶差分之间的关系。即前一阶差分与后一阶差分之间的关系，体现到帧上就是相邻三帧之间的动态关系。

## 音的四大物理属性

音高：音的高低，由物体每秒钟振动的次数决定。振动的次数越多，音越高，反之，音则越低。

音色：声音的特质。物体由于其性质、形状等的不同，即便产生相同音高的音，因相伴产生的泛音不同而有不同的音色。

音量/音强：音的强弱，由物体振动的幅度决定。振幅大音就强，振幅小音就弱。

音长：声波振动持续的时间长短。

## 分帧

语音信号是一种非平稳的时变信号，但由于发音器官的惯性运动，可以认为在一小段时间里（一般为10ms~30ms），语音信号近似不变，即语音信号具有短时平稳性。因此可以将语音信号分为一些短段来进行处理。

分帧后的每一帧的起始端和末尾端会出现不连续的地方，分帧越多与原始信号的误差越大。加窗使分帧后的信号变得连续，每一帧就会表现出周期函数的特征。在语音信号处理中，一般加汉明窗。

<http://makaidong.com/lyu0709/44493_689370_4.htm>

<https://blog.csdn.net/Tonywu2018/article/details/84678122>

<https://blog.csdn.net/yutianzuijin/article/details/77621511>

<https://www.zhihu.com/question/20136144?sort=created>

## lattice

**one best**

最初的语音识别结果只有一个，也被称为one best。语音识别解码器通过声学模型和语言模型对用户输入的语音进行打分，输出一个得分最高的结果。

![1561440096941](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1561440096941.png)

argmax就是解码器，P(X|W) 是声学模型，P(W)是语言模型。解码器是语音识别的核心，它调用声学模型和语言模型，采用维特比算法获得最优的词序列。

**lattice**

在实际的语音识别系统中，最优路径不一定与实际字序列匹配，我们一般希望能够得到得分最靠前的多条候选路径，即N-best。为了紧凑地保存候选路径，防止占用过多内存空间，一般采用lattice（词图）来保存识别的候选序列。lattice本质上是一个有向无环图。图上的每个节点代表一个词的结束时间点，每条边代表一个可能的词，以及该词发生的声学得分和语言模型得分。 

![åå§lattice](https://img-blog.csdn.net/20170827170156687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveXV0aWFuenVpamlu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在lattice上从左向右的任何一条路径，就构成一个识别结果，路径上每条边的声学得分相加，再加上路径对应的语言得分，就是整条路径的得分，通常取得分最大的前 N 条路径对应的词串作为识别的 N-Best 结果输出。上面lattice的生成用的语言模型往往不够精确，通常还需要在lattice上用更大的语言模型进rescore。

**lattice剪枝**

原始的lattice可能非常庞大，我们可以对lattice进行剪枝但不影响最终的准确率。一种剪枝方法是对lattice进行前向后向打分，计算每条边的后验概率，然后删除后验概率很低的边。对上图剪枝后获得下图：

![img](https://img-blog.csdn.net/20170827170313601?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveXV0aWFuenVpamlu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

相比原图，上面的lattice简化了不少，但是最重要的信息并没有丢失。通过计算后验概率，我们还可以很容易地知道每条边在整个lattice中的重要性。

**confusion network**

[语音识别中的lattice与confusion network](<https://blog.csdn.net/yutianzuijin/article/details/77621511>)

## Viterbi算法

**1.题目背景：**

> 从前有个村儿，村里的人的身体情况只有两种可能：健康或者发烧。
> 假设这个村儿的人没有体温计或者百度这种神奇东西，他唯一判断他身体情况的途径就是到村头我的偶像金正月的小诊所询问。
> 月儿通过询问村民的感觉，判断她的病情，再假设村民只会回答正常、头晕或冷。
> 有一天村里奥巴驴就去月儿那去询问了。
> 第一天她告诉月儿她感觉正常。
> 第二天她告诉月儿感觉有点冷。
> 第三天她告诉月儿感觉有点头晕。
> 那么问题来了，月儿如何根据阿驴的描述的情况，推断出这三天中阿驴的一个身体状态呢?
> 为此月儿上百度搜 google ，一番狂搜，发现维特比算法正好能解决这个问题。月儿乐了。



**2.已知情况：**

> 隐含的身体状态 = { 健康 , 发烧 }
>
> 可观察的感觉状态 = { 正常 , 冷 , 头晕 }
>
> 月儿预判的阿驴身体状态的概率分布 = { 健康：0.6 , 发烧： 0.4 }
>
> 月儿认为的阿驴身体健康状态的转换概率分布 = {
> 健康->健康： 0.7 ,
> 健康->发烧： 0.3 ,
> 发烧->健康：0.4 ,
> 发烧->发烧： 0.6
> }
>
> 月儿认为的在相应健康状况条件下，阿驴的感觉的概率分布 = {
> 健康，正常：0.5 ，冷 ：0.4 ，头晕： 0.1 ；
> 发烧，正常：0.1 ，冷 ：0.3 ，头晕： 0.6 
> }
> 阿驴连续三天的身体感觉依次是： 正常、冷、头晕 。



**3.题目：**

> 已知如上，求：阿驴这三天的身体健康状态变化的过程是怎么样的？



**4.过程：**

> 根据 Viterbi 理论，后一天的状态会依赖前一天的状态和当前的可观察的状态。那么只要根据第一天的正常状态依次推算找出到达第三天头晕状态的最大的概率，就可以知道这三天的身体变化情况。
> 传不了图片，悲剧了。。。
> 1.初始情况：

- P(健康) = 0.6，P(发烧)=0.4。

2.求第一天的身体情况：
计算在阿驴感觉正常的情况下最可能的身体状态。

- P(今天健康) = P(正常|健康)*P(健康|初始情况) = 0.5 * 0.6 = **0.3** 
- P(今天发烧) = P(正常|发烧)*P(发烧|初始情况) = 0.1 * 0.4 = **0.04** 

那么就可以认为第一天最可能的身体状态是：健康。
3.求第二天的身体状况：
计算在阿驴感觉冷的情况下最可能的身体状态。
那么第二天有四种情况，由于第一天的发烧或者健康转换到第二天的发烧或者健康。

- P(前一天发烧，今天发烧) = P(前一天发烧)*P(发烧->发烧)*P(冷|发烧) = 0.04 *  0.6 * 0.3 = 0.0072
- P(前一天发烧，今天健康) = P(前一天发烧)*P(发烧->健康)*P(冷|健康) = 0.04 * 0.4 * 0.4 = 0.0064
- P(前一天健康，今天健康) = P(前一天健康)*P(健康->健康)*P(冷|健康) = 0.3 * 0.7 * 0.4 =  **0.084** 
- P(前一天健康，今天发烧) = P(前一天健康)*P(健康->发烧)*P(冷|发烧) = 0.3 * 0.3 *.03 = **0.027** 

那么可以认为，第二天最可能的状态是：健康。
4.求第三天的身体状态：
计算在阿驴感觉头晕的情况下最可能的身体状态。

- P(前一天发烧，今天发烧) = P(前一天发烧)*P(发烧->发烧)*P(头晕|发烧) = 0.027 *  0.6 * 0.6 = **0.00972** 
- P(前一天发烧，今天健康) = P(前一天发烧)*P(发烧->健康)*P(头晕|健康) = 0.027 * 0.4 * 0.1 = 0.00108
- P(前一天健康，今天健康) = P(前一天健康)*P(健康->健康)*P(头晕|健康) = 0.084 * 0.7 * 0.1 =  0.00588
- P(前一天健康，今天发烧) = P(前一天健康)*P(健康->发烧)*P(头晕|发烧) = 0.084 * 0.3 *0.6 = 0.01512

那么可以认为：第三天最可能的状态是发烧。

**5.结论**

> 根据如上计算。这样月儿断定，阿驴这三天身体变化的序列是：健康->健康->发烧。



维基百科的这个例子的动态图：[File:Viterbi animated demo.gif](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/File%3AViterbi_animated_demo.gif)



这个算法大概就是通过已知的可以观察到的序列，和一些已知的状态转换之间的概率情况，通过综合状态之间的转移概率和前一个状态的情况计算出概率最大的状态转换路径，从而推断出隐含状态的序列的情况。

## beam search & viterbi

Beam Search与Viterbi算法虽然都是解空间的剪枝算法，但它们的思路是不同的。

Beam Search是对状态迁移的路径进行剪枝，而Viterbi算法是合并不同路径到达同一状态的概率值，用最大值作为对该状态的充分估计值，从而在后续计算中，忽略历史信息（这种以偏概全也就是所谓的Markov性），以达到剪枝的目的。

从状态转移图的角度来说，Beam Search是空间剪枝，而Viterbi算法是时间剪枝。

## LM与AM之间的权重问题

![1561444261738](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1561444261738.png)

## decode加速

beam search 与 Viterbi beam search

## 在线实时asr识别

![1561444545578](C:\Users\wangliyuan\AppData\Roaming\Typora\typora-user-images\1561444545578.png)

## RNN&LSTM

RNN能够

## TDNN

## Chain model

## CTC

首先，CTC不是声学模型的变化，而是优化的目标函数发生了改变，其他的神经网络（DNN，LSTM，CNN等）都可以和CTC结合。

传统的深度神经网络声学模型的训练过程是对训练数据文本做对齐得到分类的“硬判决”，并以这种“硬判决”为目标训练神经网络。这样，网络得到的结果就不是最终要识别的结果，网络优化的目标也不是最终要识别的结果。CTC是一种所见即所得的优化目标函数，训练的目标就是想要得到的结果。

传统的深度神经网络声学模型建模单元通常是state级的，而CTC的建模单元则是phone级甚至是character级的，state级反而不好。这种粗粒度的建模带来的好处就是语音数据的冗余度增加了，相邻的语音帧本来就很像并可能来自于同一个phone，那么现在就不需要那么多帧数据来建模一个句子。

通过拼帧降采样的方法可以降低数据的总帧数，在不影响识别准确率的情况下加快网络计算的速度。CTC引入了“Blank”空白，空白的引入避免了易混淆帧的“强制”对齐。**并且使得训练后的网络输出呈现“尖峰”状态，打断的Blank使得解码时通过beam的灵活调整，可以加快解码速度。**

## CNN

在12、13年的时候Ossama Abdel-Hamid就将CNN引入了语音识别中。那时候的卷积层和池化层是交替出现的，并且卷积核的规模是比较大的，CNN的层数也不是特别的多，主要用来对特征进行进一步的加工和处理，使其能够更好的被用于DNN的分类。

后来随着CNN在图像领域的发展，人们发现在图像领域中，多层卷积层之后再接池化层，减小卷积核的尺寸可以训练更深、效果更好的CNN模型。相应的方法被借鉴到了语音识别中，并根据语音识别的特点进行了进一步的优化。

IBM 的研究人员在 16 年的 ICASSP 上发表文章，称使用 3x3 的小卷积核和 多层卷积之后再接 pooling 的技术可以训练出 14 层（包含全连接）Deep CNN 模型。

[详解卷积神经网络（CNN）在语音识别中的应用](<https://www.cnblogs.com/qcloud1001/p/7941158.html?utm_source=debugrun&utm_medium=referral>)

一个简单的卷积神经网络包含一些网络层，每个网络层都通过一个可微分函数转换激活卷到新的激活卷。构建卷积神经网络需要三种主要网络层：卷积层，池化层和全连接层。

- INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.（输入是 [32x32x3]的卷，每个值代表了图像的像素值）

- CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.（连接输入卷中的局部区域，做点积，如果使用12个卷积核，则会生成 [32x32x12]的新的激活卷。对应上面的描述，这里应该会用到可微分函数来计算新的激活卷？）
- RELU layer will apply an elementwise activation function, such as the **max(0,x)** thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).（？让卷的大小不变？）
- FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.（该层的作用是计算类别分数，输出卷为[1x1x10]，这一层的每个神经元都连接上一个激活卷中的所有数值。最后分类的计算方式？SVM？Softmax？）

In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t.  In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.

这样，卷积神经网络就经过一层层的计算将原始图像的像素值转变成了最终的类别打分。注意其中有些层包含参数有些层则不包含参数。特别地，卷积层/全连接层中的转换函数用到了输入卷的激活值及神经元上的权重和偏差。而RELU层和池化层使用的是固定的函数，不需要参数。卷积层和全连接层中的参数通过梯度下降法来训练，以便网络的分类结果和训练集的标签一致。

In summary:

- A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)（将输入图像卷转换成分类结果卷）
- There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)（一般包含卷积层、全连接层、RELU层、池化层）
- Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function（每层都接收一个3维卷，并对其做变换，产生新的3维卷）
- Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)（有些层带参数，有些层不带参数）
- Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)（有些层带超参数，有些层不带超参数）

![img](http://cs231n.github.io/assets/cnn/convnet.jpeg)

卷积神经网络示例的激活过程。初始的卷存的是图像的像素值，最后的卷保存的是类别分数。每一列显示了处理路径中的每个激活卷。由于可视化3D卷很难，所以这里将卷的切片按行铺展开了。这是VGGNet结构的缩小版。



#### Convolutional Layer

The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting.

卷积层是卷积神经网络中的核心构件，它承担了大部分的计算量。

**Overview and intuition without brain stuff.** Lets first discuss what the CONV layer computes without brain/neuron analogies. The CONV layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. **Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network.** Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.

**The brain view**. If you’re a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter). We now discuss the details of the neuron connectivities, their arrangement in space, and their parameter sharing scheme.

**Local Connectivity.** When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the **receptive field** of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. **It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in space (along width and height), but always full along the entire depth of the input volume.**

![img](http://cs231n.github.io/assets/cnn/weights.jpeg)

Example filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: **If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images.** There is therefore no need to relearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.



## Highway Network

<https://blog.csdn.net/jzrita/article/details/72732037>

## Residual Network


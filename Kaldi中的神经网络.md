# Kaldi中的神经网络

在 kaldi 训练过程中，DNN 的训练是依赖于 GMM-HMM 模型的，通过 GMM-HMM 模型得到 DNN 声学模型的输出结果(在 get_egs.sh 脚本中可以看到这一过程)。因此训练一个好的 GMM-HMM 模型是 kaldi 语音识别的关键。

目前，kaldi中有三代神经网络结构，nnet1由Karel Vesely维护，nnet2由Dan Povey维护，nnet3将nnet2发展了起来。在 egs/wsj/s5/, egs/rm/s5, egs/swbd/s5 and egs/hkust/s5b中有示例代码。

nnet3的目标是支持nnet1和nnet2所支持的网络拓扑结构，并且更多；不需要太多编程，只需要配置文件就可以。

在运行DNN时，须首先训练GMM-HMM模型以得到更准确的对齐，作为输出。

## nnet1

## nnet2

## nnet3

"nnet3"不仅支持简单的前馈DNN，还可以实现在网络内层进行时间拼接（帧拼接）的时延神经网络（TDNN）并且还用于recurrent拓扑（RNN，LSTM，BLSTM等）。所以nnet3有时间轴的概念。

左右上下文

   

假设我们想要一个网络来计算特定时间索引的输出;具体来说，假设时间t=154，在网络内部进行帧拼接（或任何其他与"t"索引无关的内容），如果没有给出当前帧的左右一定范围的帧，则可能无法计算当前帧的输出。例如，如果没有看到t = 150到t = 157这个范围内的帧，则可能无法计算输出。在这种情况下（掩饰细节），我们会说网络有一个左上下文4和右上下文3。上下文的实际计算有点复杂，因为它必须考虑到特殊情况，例如奇数和偶数"t"值的情况是不同的。

   

在有recurrent拓扑的情况下，除了"所需的"左右上下文外，训练或解码时，我们还要给予"额外的"上下文。对于这种拓扑，网络可以利用超出所需上下文的上下文。在脚本中，通常会看到名为extra-left-context和extra-right-context的变量，这意味着"除了需要的内容之外，我们将提供的上下文的数量"。

   

在某些情况下，左上下文和右上下文意味着添加到块（chunk）中的总的左上下文和总的右上下文，即

   

左上下文=模型左上下文+额外左上下文

右上下文=模型右上下文+额外右上下文

   

因此，在某些情况下，您需要搞清楚一个变量指的是模型的左右上下文还是数据块的左右上下文。

   

在Kaldi5.0及更早版本中，数据块中的左右上下文不受块大小在开头或结尾的影响；在最后我们用第一或最后一帧的副本填充输入。这意味着对于recurrent拓扑，我们可能会用很多帧（最多40个左右）来填充语句的开始或结束。这没有意义而且很奇怪。在版本5.1和更高版本中，您可以指定extra-left-context-initial和extra-right-context-final，允许话语的开始/结束具有不同的上下文量。如果您指定这些值，通常将它们都指定为0（即没有额外的上下文）。但是，为了与旧版本的后台兼容，它们通常默认为-1（意思是复制默认的左上方和右上方）。

   

块大小

   

块大小是我们在训练或解码中每个数据块的（输出）帧的数量。在get_egs.sh脚本和train_dnn.py中，它也被称为frames-per-eg（在某些上下文中，这与块大小不同;见下文）。在解码中，我们把它称为frame-per-chunk。

   

非recurrent，非chain模型

   

对于使用交叉熵目标函数训练的前馈网络或TDNN等非常简单的网络类型，我们在帧级别上打乱整个数据集，并且我们一次只训练一帧。为了使大部分训练做顺序I/O，需要在帧级别上对数据进行随机化。然而，当需要10帧的左右上下文时，我们必须给出左右上下文中具体是哪些帧，当我们生成训练示例时，数据量会增加20倍左右。为了解决这个问题，我们包括一系列时间值的标签，由frame-per-eg（通常为8）控制，并包括足够的左/右上下文，我们可以在这8个帧中的任何一个上训练。然后，当我们训练模型时，任何给定的训练工作将选择其中一个8帧进行训练。

   

recurrent或chain模型

   

在RNN或LSTM模型或"Chain"模型中，我们总是训练相当大的块（通常在40到150帧的范围内）。这被称为块大小chunk-size。当我们解码时，我们还通常在相当大的数据块（如30,50或100帧）上评估神经网络。这通常被称为frames-per-chunk。对于经常性网络，我们倾向于确保在训练和解码中chunk-size/frames-per-chunk/extra-left-context和extra-right-context大致相同，因为这通常给出最佳结果（尽管有时最好使解码中的extra-context值稍大一些）。人们可能会期望在解码时间更长的情况下总是会更好，但是并不总是这样（请参见下面的looped decoding）。

   

块大小与帧 - 子采样因子的相互作用

   

在输出处有帧子采样的情况下（如链模型），块大小仍然以"t"的倍数进行测量，我们确保（通过在代码中舍入），它是帧抽样因子。请记住，如果块大小为90，帧子采样因子为3，那么我们只对每个90帧的块估计30个不同的输出索引（例如t = 0，t = 3 ... t = 87）。

### TDNN

### Chain



## 端到端

### CTC



%WER 37.09 [ 173936 / 468933, 4868 ins, 31143 del, 137925 sub ] exp/mono/decode_test//cer_10_0.0
%WER 17.98 [ 84305 / 468933, 4724 ins, 12637 del, 66944 sub ] exp/tri1/decode_test//cer_13_0.0
%WER 17.94 [ 84149 / 468933, 5025 ins, 12427 del, 66697 sub ] exp/tri2/decode_test//cer_13_0.0
%WER 17.26 [ 80945 / 468933, 4421 ins, 12958 del, 63566 sub ] exp/tri3a/decode_test//cer_14_0.0
%WER 14.16 [ 66424 / 468933, 4567 ins, 10224 del, 51633 sub ] exp/tri4a/decode_test//cer_14_0.0
%WER 12.22 [ 57304 / 468933, 4799 ins, 8197 del, 44308 sub ] exp/tri5a/decode_test//cer_14_0.0

%WER 7.15 [ 33513 / 468933, 2345 ins, 5967 del, 25201 sub ] exp/nnet3/tdnn_sp/decode_test//cer_13_0.0
[深度学习的开放数据集](<https://blog.csdn.net/zuochao_2013/article/details/79893631>)



# VoxCeleb2

牛津大学

## **摘要：**

从Youtube上下载的音视频库，包含了超过1百万条语音、超过6000位名人。是迄今最大的说话人识别数据集。

开发了基于CNN模型和训练策略的在不同条件下声纹识别方法。

结论：模型在VoxCeleb2数据集上的训练结果远比在基准数据集上的训练效果。

## **介绍：**

尽管声纹识别取得了一些研究进展，但是在噪音和不受限制的场景下有效生成说话人语音片段的单个密集向量表示依然是个挑战。本文研究了名了VGGVox的神经网络，它是CNN为基础的说话人嵌入系统，通过训练，可以将声纹语谱图映射到一个密集的欧几里得空间，该空间中的距离对应于说话人的相似度。一旦生成这样的空间，则可以直接利用我们的说话人嵌入（当作特征）进行说话人验证、聚类和分类等任务了。

在大规模人脸数据集[4,5,6]上使用深度CNN架构[1,2,3]可以有效生成人脸图像的这种映射，但声纹识别缺少大规模免费可得的自然数据集。VoxCeleb1 [7]和SITW [8]数据集是有价值的两个数据集，然而相比于流行的人脸数据集（包含上百万张图像），它们还是数据量太小。鉴于此，我们组织了VoxCeleb2，一个从开源媒体中自动获取的大规模声纹识别数据库。由于数据集是“在野外”收集的，语音片段被真实世界的噪音破坏，包括笑声、串音、通道效应、音乐和其他声音。该数据集也是多语言的，从来自145个不同民族的说话人，涵盖了不同的口音、年龄、种族和语言。该数据集是音视频的，所以可以用于其他应用，比如视觉语音合成[9,10]，语音分离[11,12]，由人脸转换成声音或由声音转换到人脸[13,14]，人脸识别等。

本文采用VGGVox来学习说话人判别性嵌入。系统包含三个主要部分：一个底层深度CNN主干网络，用于提取特征；一个池化方法用于聚合特征，为给定的语音生成单个嵌入；一个成对损失训练，用于直接优化嵌入映射本身。我们实验了基于VGG-M和ResNet的CNN主干网络。

总的来说，我们贡献了四个方面：1，组织了比其他声纹识别数据集都大的大规模数据集；它弥补了VoxCeleb1中种族缺乏多样性的不足。2，对于声纹识别，我们提出了适于语谱图输入的ResNet网络结构。3，在VoxCeleb1数据集中，使用我们的嵌入打败了最新的说话人验证。4，提出了新的验证基准测试集（VoxCeleb1）。

## **相关工作：**

传统方法：声纹识别一直被i-vectors统治，分类方法有重尾分布PLDA和高斯PLDA。它的缺点是依赖于手工生成的特征工程hand-crafted feature engineering。传统方法的综述[20].

深度学习方法：在计算机视觉和语音识别领域的成功激发了DNN作为特征提取器并联合分类器的使用，尽管这不是端到端的。尽管上面的方式很高效，它们依然需要hand-crafted feature engineering。然而，CNN可以直接应用到原始的语谱图中，以端到端的方式训练。比如[26]使用孪生前向网络来判别性地比较两个语音，然而这种方法依赖于预先计算的MFCC特征。[27]也需要学习特征。与我们的工作最接近的是[28]，它使用三元组损失triplet loss训练神经嵌入系统。但他们使用私有内部的数据集来训练和验证，所以无法和他们的工作作比较。

数据集：目前存在的声纹识别数据集通常具有以下一个或几个局限：1，它们要么是在受控制的条件（比如电话语音或声学实验室）下获得。2，它们手工标注，所以限制了语料集规模。3，用户无法免费获取。而VoxCele2数据集打破了这些限制。

## VoxCeleb2数据集

### 描述

VoxCeleb2从上传到YouTube的视频中提取了6000多名名人的100多万句话。数据集性别相当平衡，61%的演讲者是男性。演讲者来自不同的种族、口音、职业和年龄。数据集中包含的视频是在大量具有挑战性的视觉和听觉环境中拍摄的。这些包括在红地毯上的采访，在室外体育场和安静的室内工作室，对大量观众的演讲，专业拍摄的多媒体片段，甚至是用手持设备拍摄的粗糙视频。数据集中的音频片段会随着背景的震动、笑声、重叠的语音和不同的房间音响效果而退化。我们还为数据集中的说话者提供人脸检测和人脸轨迹，人脸图像也类似于野外，在姿态(包括轮廓)、光照、图像质量和运动模糊方面存在差异。表1给出了总体统计数据，图1展示了裁剪后的人脸以及说话长度、性别和民族分布的例子。

数据集包含开发(train/val)和测试集。然而，由于我们使用VoxCeleb1数据集进行测试，因此只有开发集将用于说话人识别任务(第4和第5节)。分割如表2所示。VoxCeleb2的开发集与VoxCeleb1或SITW数据集中的标识没有重叠。

### 采集流程

我们使用自动计算机视觉管道来管理VoxCeleb2。虽然这条管道类似于用来编译VoxCeleb1[7]的管道，但细节已经进行了修改，以提高效率，并允许从多个姿势识别会说话人面孔，而不仅仅是近正面的姿势。事实上，我们改变了管道的每个关键组件的实现:人脸检测器、人脸跟踪器、用于执行主动说话人验证的SyncNet模型，以及最后的人脸识别模型。我们还为自动删除重复添加了一个附加步骤。这个管道允许我们获得一个五倍于[7]大小的数据集。我们还注意到，名人姓名列表涵盖了更广泛的国籍，因此与[7]不同，获得的数据集是多语言的。为了清楚起见，以下各段将讨论关键阶段：

**阶段1：**感兴趣人士候选人名单(POIs)。第一步是获取POIs列表。我们从VGGFace2数据集[4]中出现的人员列表开始，该数据集具有相当大的种族多样性和职业多样性。这份名单包含了9000多个身份，从演员、运动员到政治家。与VoxCeleb1和SITW重叠的标识将从开发集中删除。

**阶段2：**下载视频。每个POIs的前100个视频都会通过YouTube搜索自动下载。在搜索查询中，单词interview被附加到POI的名称后面，以增加视频包含POI说话的可能性，而不是体育或音乐视频。

**阶段3：**人脸跟踪。基于单镜头多盒检测器(SSD)[35]的CNN人脸检测器用于检测视频每帧的人脸外观。与[7]相比，该检测器有明显的改进，可以检测侧面和极端姿势中的人脸。我们使用了基于ROI重叠的与[7]相同的跟踪器。

**阶段4：**人脸验证。人脸识别CNN用于将人脸跟踪的结果分类为是否属于POI。这里使用的分类网络基于在VGGFace2数据集上训练的ResNet- 50网络[16]。验证是通过直接使用这个分类分数来完成的。

**阶段5：**主动说话者验证。这个阶段的目标是确定是否可见的面孔是说话者。这是通过使用SyncNet[37,38]的多视图自适应[36]实现的，这是一种双流CNN，通过估计音频轨迹和视频嘴部运动之间的相关性来确定主动说话者。该方法可以拒绝包含配音或画外音的剪辑。

**阶段6：**重复的删除。使用YouTube作为视频来源的一个警告是，通常相同的视频(或视频的一部分)可以上传两次，尽管url不同。重复的识别和删除如下:每个语音段由一个1024D向量表示，使用[7]中的模型作为特征提取器。欧几里得距离是计算同一说话人的所有特征对之间的距离。如果任何两个语音段之间的距离小于一个非常保守的阈值(0:1)，则认为语音段是相同的，并删除其中一个。这种方法肯定能识别所有精确的重复，在实践中我们发现，它也能成功地识别近重复，例如，同一源的语音段被不同地裁剪。

**阶段7：**获得国籍标签。从维基百科上为数据集中的所有名人抓取国籍标签。我们爬向国籍国家，而不是种族，因为这往往更能体现口音。除428名被标记为未知外，所有人都获得了国籍标签。数据集中的说话者来自145个国家(VoxCeleb1为36个)，从而产生了一个更加具有种族多样性的数据集(见图1(右下)民族分布)。还要注意，使用VoxCeleb2(29%)的美国人所占比例要小于使用VoxCeleb1(64%)的美国人。

**讨论：**为了确保我们的系统对人体被正确识别，并且说话过程中没有手动的干扰，很有自信，我们设定保守的阈值，以尽量减少FP的数目。由于VoxCeleb2主要设计为一个仅供训练的数据集，因此与用于编译VoxCeleb1的阈值相比，阈值没有那么严格，因此丢弃的视频更少。尽管如此，在手工检查数据集的一个重要子集之后，我们只发现了很少的标签错误。

## VGGVox

[安装Matlab](<https://blog.csdn.net/qq_32892383/article/details/79670871>)